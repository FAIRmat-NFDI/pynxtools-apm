#
# Copyright The NOMAD Authors.
#
# This file is part of NOMAD. See https://nomad-lab.eu for further info.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

"""Utilities for overwriting h5py chunking heuristic to consider domain-specific access pattern."""

import numpy as np
from pynxtools.dataconverter.chunk_cache import CHUNK_CONFIG_DEFAULT

from pynxtools_apm.utils.custom_logging import logger


def prioritized_axes_heuristic(
    data: np.ndarray,
    priority: tuple[int, ...],
) -> tuple[int, ...] | bool:
    """Define an explicit tuple[int] how to chunk data with shape

    Parameter:
    * data, a numpy array
    * priority, all dimension scale axes indices, arranged
    in increasing priority how conservative axes should be splitted into chunks.
    the later the index in priority, the more likely this axis will be splitted
    into fewer chunks, if any

    Examples substantiating this heuristic:
    * Electron microscopy, a stack of 100,000 x 1024 x 1024 2D images, 4B int itemsize:
    Chunking should keep dim 1 and especially dim 2 (especially for C-style storage, i.e.,
    when dim 2 changes fastest, as contiguous as possible, and chunk mainly on dim 0,
    Reason is often users wish to read entire images completely than slicing thin
    pixel slabs in other (orthogonal directions)
    * Reconstructed ion positions in atom probe 1,000,000 x 3, 4B itemsize,
    users wish to read entire position triplets in contiguous blocks of triplets
    rather than reading fast a single column. Therefore, dim 0 should be chunked
    with higher priority while trying to keep dim 1 intact.

    By contrast, h5py via guess_chunk function e.g., https://github.com/h5py/h5py/blob/
    706755340058c8e8000ed769d4f5ad3571e4dfce/h5py/_hl/filters.py#L361
    splits alternatingly across all dims. In effect, arrays get chunked more regularly
    like above mention can easily cause that even a single 1024 x 1024
    image will be distributed across dozens of chunk_shape, mind that
    guess_chunk is a heuristic ought to apply to general cases,
    it must not be understood as the solution to go with if the
    usage pattern of an array is well-known and more specific
    and known read-out are used frequently. Currently, guess_chunk offers
    a compromise for slicing about equally all three orthogonal
    directions.

    Returns value for the chunks parameter of h5py create_dataset
    * tuple[int, ...], explicit chunk size
    * True, replying on h5py guess_chunk auto-chunking via chunk_shape=True."""
    if not isinstance(data, np.ndarray):  # only np.ndarray supported
        logger.warning(f"chunk strategy, auto, for non-numpy array")
        return True
    shape: tuple[int, ...] = np.shape(data)
    if any(extent == 0 for extent in shape):  # unlimited axis not supported
        logger.warning(f"chunk strategy, auto, for datasets with unlimited axes")
        return True
    if set(priority) != set(range(len(priority))):  # all dim indices need to be present
        logger.warning(f"chunk strategy, auto, incorrect axes priority setting")
        return True
    if len(shape) == 0:
        raise ValueError("chunk_shape not allowed for scalar datasets.")
        # also h5py by default would raise in such a case
    chunk_shape: list[float] = list(float(extent) for extent in shape)
    max_byte_per_chunk: int = CHUNK_CONFIG_DEFAULT["byte_size"]
    byte_per_item: int = data.itemsize

    dim = 0
    idx = 0
    logger.info(
        f"chunk strategy, prioritized_axes_heuristic analyzing for shape {shape} and byte_per_item {byte_per_item} ..."
    )  # allow monitoring if we ever run into an infinite loop
    while True:
        idx += 1
        byte_per_chunk = np.prod(chunk_shape) * byte_per_item
        logger.info(
            f"chunk strategy, while {idx}, {dim}, {chunk_shape}, {byte_per_chunk}"
        )
        if byte_per_chunk < max_byte_per_chunk:
            break
        if chunk_shape[dim] % 2 == 0:
            chunk_shape[dim] = chunk_shape[dim] / 2
        else:
            chunk_shape[dim] = (chunk_shape[dim] / 2) + 1

        if dim < (len(shape) - 1):
            if chunk_shape[dim] < 2:
                dim += 1
                # seems we cannot reduce byte_per_chunk further by splitting
                # along dim, so unfortunately need to consider splitting across
                # the next, less prioritized axis
        else:
            # continue splitting on the same axes irrespective if ndims == 1 or higher
            if chunk_shape[dim] >= 2:
                continue

            # can't figure something out that is anywhere smarter, go with h5py chunking heuristic
            logger.info(
                f"chunk strategy, auto, no more axes can be splitted to reduce byte_per_chunk"
            )
            return True

    if all(int(extent) >= 1 for extent in chunk_shape):
        logger.info(
            f"chunk strategy, {tuple(int(extent) for extent in chunk_shape)} for shape {shape} with byte_per_item {byte_per_item} using chunk_shape {chunk_shape}, byte_per_chunk {byte_per_chunk}"
        )
        return tuple(int(extent) for extent in chunk_shape)
    logger.info(f"chunk strategy, auto")
    return True


# retval = prioritized_axes_heuristic((100000, 2048, 2048), (0, 1, 2), 8)
# print(retval)
# retval = prioritized_axes_heuristic((1000000, 3), (0, 1), 4)
# print(retval)
# retval = prioritized_axes_heuristic((60, 60, 180), (2, 1, 0), 4)
# print(retval)
# retval = prioritized_axes_heuristic((1, 1, 1), (0, 1, 2), 4)
# print(retval)
