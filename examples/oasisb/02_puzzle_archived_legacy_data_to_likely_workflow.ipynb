{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037feae-7514-4fa1-a81e-d96eec2ff262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pynxtools_apm.utils.versioning import __version__\n",
    "from pynxtools_apm.examples.get_sha256_of_directories import SEPARATOR\n",
    "from pynxtools_apm.examples.oasisb_utils import APT_MIME_TYPES, CAMECA_ROOT_MIME_TYPES\n",
    "\n",
    "# TODO parameterize\n",
    "with open(\"root_directory.txt\") as fp:\n",
    "    src_directory = f\"{fp.read().strip().replace('/', os.sep)}\"\n",
    "print(src_directory)\n",
    "\n",
    "header = \"file_path:archive_path;byte_size;unix_mtime;sha256sum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebfe93-ffd1-403f-9da9-792b517478e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config: dict[str, str] = {\n",
    "    \"python_version\": f\"{sys.version.replace(' ', '_')}\",\n",
    "    \"working_directory\": f\"{os.getcwd()}\",\n",
    "    \"pynxtools_apm version\": f\"{__version__}\",\n",
    "    # \"directory\": f\"src_directory,  # sys.argv[1],\n",
    "}\n",
    "\n",
    "ods = pd.read_excel(\n",
    "    f\"{src_directory}{os.sep}data{os.sep}aaa_legacy_data.ods\",\n",
    "    sheet_name=\"aaa_legacy_data\",\n",
    "    engine=\"odf\",\n",
    ")\n",
    "cnt: dict[int, int] = {1: 0, 2: 0}\n",
    "just_display_files_from_project = False\n",
    "\n",
    "for row in ods.itertuples(index=True):\n",
    "    if row.project_name not in (\"mar_tomographic_hellman\"):\n",
    "        continue\n",
    "    if row.parse in (1, 2):\n",
    "        print(f\"project{SEPARATOR}{row.project_name}{SEPARATOR}hashing...\")\n",
    "        sub_directory = f\"{src_directory}{os.sep}data{os.sep}{row.project_name}\"\n",
    "        project_csv = (\n",
    "            f\"{src_directory}{os.sep}data{os.sep}{row.project_name}.sha256.results.csv\"\n",
    "        )\n",
    "        with open(project_csv, \"r\") as fp:\n",
    "            start = next(idx for idx, line in enumerate(fp) if header in line)\n",
    "        df_hash = pd.read_csv(project_csv, sep=\";\", skiprows=start)\n",
    "        del start, project_csv\n",
    "        df_hash.columns = [\"path\", \"size\", \"mtime\", \"sha256\"]\n",
    "        # print(df_hash)\n",
    "\n",
    "        # multiple checks for consistency\n",
    "        # i) which files with atom-probe-specific mimetype are listed in df ...\n",
    "        hash_to_file: dict[str, list[str]] = {}\n",
    "        for line in df_hash.itertuples(index=True):\n",
    "            if line.path.lower().endswith(\n",
    "                tuple(APT_MIME_TYPES + CAMECA_ROOT_MIME_TYPES)\n",
    "            ):\n",
    "                # ... and which of these are unique?\n",
    "                if line.sha256 not in hash_to_file:\n",
    "                    hash_to_file[line.sha256] = [line.path]\n",
    "                else:\n",
    "                    hash_to_file[line.sha256].append(line.path)\n",
    "                    # print(f\"WARNING::{line.path} is almost surely a duplicate\")\n",
    "        del df_hash\n",
    "        # summarize these uniques, for the demo we only want to have a dataset in NOMAD ones per project\n",
    "        file_to_hash: dict[str, str] = {}\n",
    "        # file_has_likely_content_duplicate: dict[str, bool] = {}\n",
    "        for hash_value, file_name_list in hash_to_file.items():\n",
    "            for file_name in file_name_list:\n",
    "                if file_name not in file_to_hash:\n",
    "                    file_to_hash[file_name] = hash_value\n",
    "                    # file_has_likely_content_duplicate[file_name] = True if len(file_name_list) > 1 else False\n",
    "                else:\n",
    "                    print(f\"{line.path}, {file_name} has already been added before\")\n",
    "        # del hash_to_file\n",
    "\n",
    "        if just_display_files_from_project:\n",
    "            for file_name in file_to_hash:\n",
    "                print(file_name)\n",
    "            continue\n",
    "\n",
    "        project_ods = f\"{src_directory}{os.sep}data{os.sep}{row.project_name}.ods\"\n",
    "        df_workflow = pd.read_excel(\n",
    "            project_ods,\n",
    "            sheet_name=row.project_name,\n",
    "            engine=\"odf\",\n",
    "        ).fillna(\"\")\n",
    "        del project_ods\n",
    "        # print(df_workflow)\n",
    "\n",
    "        # iii) are all the files that are referred to in each row, i.e., puzzled together workflow, in f\"{row.project_name}.ods\" listed in df?\n",
    "        n_entries = 0\n",
    "        composite_hashes: set[str] = set()\n",
    "        for line in df_workflow.itertuples(index=True):\n",
    "            has_entry = False\n",
    "            composite_hash = \"\"\n",
    "            for col_name in [\n",
    "                \"str_rraw\",\n",
    "                \"rhit_hits\",\n",
    "                \"root\",\n",
    "                \"pos_epos_apt_ato_csv\",\n",
    "                \"rng_rrng_fig_env\",\n",
    "                \"hdf_xml_nxs_raw_ops\",\n",
    "            ]:\n",
    "                value = getattr(line, col_name)\n",
    "                if value != \"\":\n",
    "                    if value in file_to_hash:\n",
    "                        has_entry = True\n",
    "                        composite_hash += f\"{file_to_hash[value]}_\"\n",
    "                        # need to work with duplicate instead?\n",
    "                        # if value in file_has_likely_content_duplicate:\n",
    "                        #     if file_has_likely_content_duplicate[value]:\n",
    "                        #         if f\"{value}\" != f\"{hash_to_file[file_to_hash[value]][0]}\":\n",
    "                        #             print(f\">>>> REPLACE {value} by {hash_to_file[file_to_hash[value]][0]}\")\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\">>>> {value} from column {col_name} has not been hashed\"\n",
    "                        )\n",
    "            if has_entry:\n",
    "                if composite_hash not in composite_hashes:\n",
    "                    composite_hashes.add(composite_hash)\n",
    "                    n_entries += 1\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"REMOVE {line.Index} from {row.project_name}.ods as it will likely create an entry containing duplicated data\"\n",
    "                    )\n",
    "            del has_entry, composite_hash\n",
    "        del composite_hashes\n",
    "        print(f\">>>> {row.project_name} has {n_entries} entries\")\n",
    "        if n_entries == 0:\n",
    "            print(\"\\n\")\n",
    "            for file_name in file_to_hash:\n",
    "                print(f\"{file_name}\")\n",
    "            print(\"\\n\")\n",
    "        cnt[row.parse] += n_entries\n",
    "        del n_entries\n",
    "        del file_to_hash, hash_to_file  # file_has_likely_content_duplicate\n",
    "print(f\"{cnt} entries expected in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4675ca-cd47-4b2d-a51f-afb322024190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eeb233-1232-43b2-9926-05ac2edc57ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3faf58-6568-4062-a017-3660ff55af50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
