{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd30f56-ecab-4111-8c78-7ec1eb05d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from odf.opendocument import load\n",
    "from odf.table import Table\n",
    "from odf.element import Element\n",
    "import pandas as pd\n",
    "from pynxtools_apm.examples.oasisb_utils import (\n",
    "    is_valid_alpha3,\n",
    "    snake_case_to_camel_case,\n",
    ")\n",
    "import bibtexparser\n",
    "\n",
    "# TODO parameterize\n",
    "src_directory = \"CHANGEME\".replace(\"/\", os.sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a61349-48de-4faa-956b-ff9b008dc109",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reformat the ods as that had successively grown becoming unmanagable, split these sheets up and standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393325aa-b582-444c-b9ce-4ef672e196f1",
   "metadata": {},
   "source": [
    "* For each project sheet we do not want to have blank rows to concatenate the project sub-directory, sheet row_id for making entries and uploads be unique and descriptive.*\n",
    "* sheets originally where all in one spreadsheet, which grew and become difficult to navigate, also not the typical use case, multiple people would puzzle together knowledge<br>\n",
    "* Therefore, we disentangle here the original spreadsheet and clean it from blanks plus some more harmonization of formatting to reflect newer parsing capabilities for more<br>\n",
    "  infrequently represented community file formats, typical NOMAD upload then aus_sydney_breen with entries named aus_sydney_breen_{1, 2, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ed96a-0905-4ffe-8b3c-db7d193f4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sorting = False\n",
    "if run_sorting:\n",
    "    doc = load(f\"{src_directory}{os.sep}harvest.examples.14.apm.xls.ods\")\n",
    "\n",
    "    spreadsheet = doc.spreadsheet\n",
    "    sheets = [\n",
    "        sheet\n",
    "        for sheet in spreadsheet.childNodes\n",
    "        if getattr(sheet, \"tagName\", \"\") == \"table:table\"\n",
    "    ]\n",
    "    sorted_sheets = sorted(sheets, key=lambda s: s.getAttribute(\"name\"))\n",
    "    for sheet in sheets:\n",
    "        spreadsheet.removeChild(sheet)\n",
    "    for sheet in sorted_sheets:\n",
    "        spreadsheet.appendChild(sheet)\n",
    "\n",
    "    doc.save(f\"{src_directory}{os.sep}legacy_data_tmp.ods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c6854-2cc0-4ba9-8799-954c3dd360f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sheet_names, modify these individually and export to project-specific file\n",
    "# as the initial idea to have the sheets all together was simple when the collection\n",
    "# was not that large but it becomes more and more impractical to navigate this\n",
    "# large ODS sheet\n",
    "doc = load(f\"{src_directory}{os.sep}legacy_data_tmp.ods\")\n",
    "spreadsheet = doc.spreadsheet\n",
    "sheet_names = [\n",
    "    node.getAttribute(\"name\")\n",
    "    for node in spreadsheet.childNodes\n",
    "    if getattr(node, \"tagName\", \"\") == \"table:table\"\n",
    "]\n",
    "# print(sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2318fe4-c35e-45ce-ad20-0f7d9f4f0c89",
   "metadata": {},
   "source": [
    "Splitting it up makes also sense because legacy data mappings would typically be<br>\n",
    "contributed efforts by multiple people. Which is what we are here mimicking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50cb672-28e0-4c85-a7b2-7f6025e96283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_splitting = False\n",
    "if run_splitting:\n",
    "    column_header = [\n",
    "        \"str_rraw\",\n",
    "        \"rhit_hits\",\n",
    "        \"root\",\n",
    "        \"pos_epos_apt_ato_csv\",\n",
    "        \"rng_rrng_fig_env\",\n",
    "        \"hdf_xml_nxs_raw_ops\",\n",
    "    ]\n",
    "    for sheet_name in sheet_names[1:]:\n",
    "        if sheet_name != \"aaa_legacy_data\":\n",
    "            print(f\"Validating {sheet_name}\")\n",
    "            df = pd.read_excel(\n",
    "                f\"{src_directory}{os.sep}legacy_data_tmp.ods\",\n",
    "                sheet_name=sheet_name,\n",
    "                engine=\"odf\",\n",
    "            )\n",
    "            df = df.fillna(\"\")\n",
    "            df = df[~((df.isna() | (df == \"\")).all(axis=1))]  # keep non-blank rows only\n",
    "\n",
    "            # equalize number of columns\n",
    "            if len(column_header) > df.shape[1]:  # needs adding empty columns\n",
    "                for idx in range(0, len(column_header) - df.shape[1]):\n",
    "                    df[f\"new_column_{idx}\"] = \"\"\n",
    "                df.columns = column_header + [\n",
    "                    f\"ignore{col_idx}\"\n",
    "                    for col_idx in range(0, len(column_header) - df.shape[1])\n",
    "                ]\n",
    "            elif df.shape[1] > len(column_header):  # needs expanding the column_header\n",
    "                df.columns = column_header + [\n",
    "                    f\"ignore{col_idx}\"\n",
    "                    for col_idx in range(0, df.shape[1] - len(column_header))\n",
    "                ]\n",
    "            else:\n",
    "                df.columns = column_header + [\n",
    "                    f\"ignore{col_idx}\"\n",
    "                    for col_idx in range(0, df.shape[1] - len(column_header))\n",
    "                ]\n",
    "\n",
    "            # report issues with incorrectly categorized datasets\n",
    "            for idx, row in df.iterrows():\n",
    "                for col_name in df.columns:\n",
    "                    if row[col_name] != \"\" and not col_name.startswith(\"ignore\"):\n",
    "                        if row[col_name][\n",
    "                            row[col_name].rfind(\".\") + 1 :\n",
    "                        ].lower() not in col_name.split(\"_\"):\n",
    "                            print(\n",
    "                                f\"WARNING, {sheet_name}, {idx}, {col_name}, {row[col_name]}\"\n",
    "                            )\n",
    "                        # else:\n",
    "                        #     print(f\"{idx}, {col_name}, OK\")\n",
    "\n",
    "            df.to_excel(\n",
    "                f\"{src_directory}{os.sep}data{os.sep}{sheet_name}.ods\",\n",
    "                sheet_name=sheet_name,\n",
    "                engine=\"odf\",\n",
    "                index=False,\n",
    "            )\n",
    "            del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485473f-cd89-45dc-a611-0cbc4fcc8ccb",
   "metadata": {},
   "source": [
    "### Validate formatting of the legacy data and that sufficient bibliographical metadata is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248501b-4904-4392-a0de-e6f0cdf37085",
   "metadata": {},
   "source": [
    "Do we have for each sub-directory also a project file and vice versa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe614f-e167-4265-bd05-d04ff9403089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bibliographical_metadata(\n",
    "    bib: dict, snake_case_project_name: str\n",
    ") -> list[str, str]:\n",
    "    \"\"\"Get dataset and article citation_key for given project.\"\"\"\n",
    "    matching: dict[str, list[str]] = {\n",
    "        \"data\": [],\n",
    "        \"paper\": [],\n",
    "    }\n",
    "    camel_case_project_name = snake_case_to_camel_case(snake_case_project_name)\n",
    "    for key in bib:\n",
    "        for prefix, cls in [(\"D_\", \"data\"), (\"A_\", \"paper\")]:\n",
    "            if key == f\"{prefix}{camel_case_project_name}\":\n",
    "                matching[cls].append(key)\n",
    "    data_article: list[str, str] = [\"\", \"\"]\n",
    "    for idx, cls, entry_type in [\n",
    "        (0, \"data\", \"an original dataset\"),\n",
    "        (1, \"paper\", \"an original research article\"),\n",
    "    ]:\n",
    "        if len(matching[cls]) == 0:\n",
    "            # if cls == \"data\":\n",
    "            print(f\"ERROR, {snake_case_project_name} has no reference for {entry_type}\")\n",
    "            # print(f\"@Misc{{D_{camel_case_project_name}}},\\n  author={{}},\\n note = {{personal communication}},\\n year = {{2024}},\\n}},\")\n",
    "        elif len(matching[cls]) > 1:\n",
    "            print(\n",
    "                f\"WARNING, {snake_case_project_name} has more than one reference for {entry_type}\"\n",
    "            )\n",
    "        else:\n",
    "            data_article[idx] = matching[cls][0]\n",
    "    return data_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac91a36-377f-4f06-ae92-23e927a5115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods = pd.read_excel(\n",
    "    f\"{src_directory}{os.sep}data{os.sep}aaa_legacy_data.ods\",\n",
    "    sheet_name=\"aaa_legacy_data\",\n",
    "    engine=\"odf\",\n",
    ")\n",
    "with open(f\"{src_directory}{os.sep}data{os.sep}aaa_legacy_data.bib\") as fp:\n",
    "    bib = bibtexparser.load(fp).entries_dict\n",
    "\n",
    "for row in ods.itertuples(index=True):\n",
    "    if row.parse in (1, 2):\n",
    "        # print(row.project_name)\n",
    "        if not is_valid_alpha3(row.project_name[0 : row.project_name.find(\"_\")]):\n",
    "            print(f\"{row.project_name} has invalid country alpha3 code\")\n",
    "            continue\n",
    "        prefix = f\"{src_directory}{os.sep}data{os.sep}\"\n",
    "        if not os.path.isdir(f\"{prefix}{row.project_name}\"):\n",
    "            print(f\"{row.project_name} has no sub-directory\")\n",
    "            continue\n",
    "        if not os.path.isfile(f\"{prefix}{row.project_name}.ods\"):\n",
    "            print(f\"{row.project_name} has no valid ods file\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_excel(\n",
    "                f\"{prefix}{row.project_name}.ods\",\n",
    "                sheet_name=row.project_name,\n",
    "                engine=\"odf\",\n",
    "            )\n",
    "            del df\n",
    "        except ValueError:\n",
    "            print(f\"{row.project_name} ods sheet has an inconsistent sheet_name\")\n",
    "        data, article = get_bibliographical_metadata(bib, row.project_name)\n",
    "        # print(f\"{row.project_name}, __{data}__, __{article}__\")\n",
    "print(\"Integriety check performed.\")\n",
    "# for sheet_name in sheet_names[1:]:\n",
    "#     sub_directory = f\"{src_directory}{os.sep}data{os.sep}{sheet_name}\"\n",
    "\n",
    "#     print(sheet_name)\n",
    "# print(f\"{sub_directory} {'OK' if os.path.isdir(sub_directory) else 'MISSING'}\")\n",
    "# if not os.path.isdir(sub_directory):\n",
    "#     print(f\"{sub_directory} MISSING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4956e3-0d73-4e43-af82-de6d2adbcc91",
   "metadata": {},
   "source": [
    "### Check that bibliography matches in formatting and content to the legacy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21c135-3329-4454-94d9-32f52957b7e5",
   "metadata": {},
   "source": [
    "SnakeCase sub-directory names matching CamelCase citation keys, assure one D_ entry for each sub-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9e011-a6a7-4dc2-9114-adfa9da603ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in manual atom_types for all those datasets for which no associated ranging definitions file was share, e.g., morgado, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa108cb-bbd4-4030-9497-67d491c0a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nice to have would be to remove duplicated files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
